<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Deploy a Model on Amazon SageMaker Hosting Services</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deploy a Model on Amazon SageMaker Hosting Services<a name="how-it-works-hosting"></a></h1>
</header>
<p>Amazon SageMaker also provides model hosting services for model deployment, as shown in the following diagram. Amazon SageMaker provides an HTTPS endpoint where your machine learning model is available to provide inferences.</p>
<figure>
<img src="http://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture.png" alt="" /><figcaption>[Image NOT FOUND]</figcaption>
</figure>
<p>Deploying a model using Amazon SageMaker hosting services is a three-step process:</p>
<ol type="1">
<li><p><strong>Create a model in Amazon SageMaker</strong>—By creating a model, you tell Amazon SageMaker where it can find the model components. This includes the S3 path where the model artifacts are stored and the Docker registry path for the image that contains the inference code. In subsequent deployment steps, you specify the model by name. For more information, see the <a href="API_CreateModel.md">CreateModel</a> API.</p></li>
<li><p><strong>Create an endpoint configuration for an HTTPS endpoint</strong>—You specify the name of one or more models in production variants and the ML compute instances that you want Amazon SageMaker to launch to host each production variant.</p>
<p>When hosting models in production, you can configure the endpoint to elastically scale the deployed ML compute instances. For each production variant, you specify the number of ML compute instances that you want to deploy. When you specify two or more instances, Amazon SageMaker launches them in multiple Availability Zones. This ensures continuous availability. Amazon SageMaker manages deploying the instances. For more information, see the <a href="API_CreateEndpointConfig.md">CreateEndpointConfig</a> API.</p></li>
<li><p><strong>Create an HTTPS endpoint</strong>—Provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration. For more information, see the <a href="API_CreateEndpoint.md">CreateEndpoint</a> API. To get inferences from the model, client applications send requests to the Amazon SageMaker Runtime HTTPS endpoint. For more information about the API, see the <a href="API_runtime_InvokeEndpoint.md">InvokeEndpoint</a> API.</p></li>
</ol>
<p><strong>Note</strong><br />
When you create an endpoint, Amazon SageMaker attaches an Amazon EBS storage volume to each ML compute instance that hosts the endpoint. The size of the storage volume depends on the instance type. For a list of instance types that Amazon SageMaker hosting service supports, see <a href="https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html#limits_sagemaker">AWS Service Limits</a>. For a list of the sizes of the storage volumes that Amazon SageMaker attaches to each instance, see <a href="host-instance-storage.md">Hosting Instance Storage Volumes</a>.</p>
<p>To increase a model’s accuracy, you might choose to save the user’s input data and ground truth, if available, as part of the training data. You can then retrain the model periodically with a larger, improved training dataset.</p>
<p>When hosting models using Amazon SageMaker hosting services, consider the following: + Typically, a client application sends requests to the Amazon SageMaker HTTPS endpoint to obtain inferences from a deployed model. You can also send requests to this endpoint from your Jupyter notebook during testing.</p>
<p>  + You can deploy a model trained with Amazon SageMaker to your own deployment target. To do that, you need to know the algorithm-specific format of the model artifacts that were generated by model training. For more information about output formats, see the section corresponding to the algorithm you are using in <a href="cdf-training.md#td-serialization">Training Data Formats</a>.</p>
<p>  + You can deploy multiple variants of a model to the same Amazon SageMaker HTTPS endpoint. This is useful for testing variations of a model in production. For example, suppose that you’ve deployed a model into production. You want to test a variation of the model by directing a small amount of traffic, say 5%, to the new model. To do this, create an endpoint configuration that describes both variants of the model. You specify the <code>ProductionVariant</code> in your request to the <code>CreateEndPointConfig</code>. For more information, see <a href="API_ProductionVariant.md">ProductionVariant</a>.</p>
<p>  + You can configure a <code>ProductionVariant</code> to use Application Auto Scaling. For information about configuring automatic scaling, see <a href="endpoint-auto-scaling.md">Automatically Scale Amazon SageMaker Models</a>.</p>
<p>  + You can modify an endpoint without taking models that are already deployed into production out of service. For example, you can add new model variants, update the ML Compute instance configurations of existing model variants, or change the distribution of traffic among model variants. To modify an endpoint, you provide a new endpoint configuration. Amazon SageMaker implements the changes without any downtime. For more information see, <a href="API_UpdateEndpoint.md">UpdateEndpoint</a> and <a href="API_UpdateEndpointWeightsAndCapacities.md">UpdateEndpointWeightsAndCapacities</a>.</p>
<p>  + Changing or deleting model artifacts or changing inference code after deploying a model produces unpredictable results. If you need to change or delete model artifacts or change inference code, modify the endpoint by providing a new endpoint configuration. Once you provide the new endpoint configuration, you can change or delete the model artifacts corresponding to the old endpoint configuration.</p>
<p>  + If you want to get inferences on entire datasets, consider using batch transform as an alternative to hosting services. For information, see <a href="how-it-works-batch.md">Get Inferences for an Entire Dataset with Batch Transform</a></p>
<p><a href="how-it-works-model-validation.md">Validate a Machine Learning Model</a></p>
</body>
</html>
