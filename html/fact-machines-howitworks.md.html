<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>How Factorization Machines Work</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">How Factorization Machines Work<a name="fact-machines-howitworks"></a></h1>
</header>
<p>The prediction task for a factorization machine model is to estimate a function ŷ from a feature set xi to a target domain. This domain is real-valued for regression and binary for classification. The factorization machine model is supervised and so has a training dataset (xi,yj) available. The advantages this model presents lie in the way it uses a factorized parametrization to capture the pairwise feature interactions. It can be represented mathematically as follows:</p>
<figure>
<img src="http://docs.aws.amazon.com/sagemaker/latest/dg/images/FM1.jpg" alt="" /><figcaption>[An image containing the equation for the factorization machine model.]</figcaption>
</figure>
<p>The three terms in this equation correspond respectively to the three components of the model: + The w0 term represents the global bias. + The wi linear terms model the strength of the ith variable. + The &lt;vi,vj&gt; factorization terms model the pairwise interaction between the ith and jth variable.</p>
<p>The global bias and linear terms are the same as in a linear model. The pairwise feature interactions are modeled in the third term as the inner product of the corresponding factors learned for each feature. Learned factors can also be considered as embedding vectors for each feature. For example, in a classification task, if a pair of features tends to co-occur more often in positive labeled samples, then the inner product of their factors would be large. In other words, their embedding vectors would be close to each other in cosine similarity. For more information about the factorization machine model, see <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">Factorization Machines</a>.</p>
<p>For regression tasks, the model is trained by minimizing the squared error between the model prediction ŷn and the target value yn. This is known as the square loss:</p>
<figure>
<img src="http://docs.aws.amazon.com/sagemaker/latest/dg/images/FM2.jpg" alt="" /><figcaption>[An image containing the equation for square loss.]</figcaption>
</figure>
<p>For a classification task, the model is trained by minimizing the cross entropy loss, also known as the log loss:</p>
<figure>
<img src="http://docs.aws.amazon.com/sagemaker/latest/dg/images/FM3.jpg" alt="" /><figcaption>[An image containing the equation for log loss.]</figcaption>
</figure>
<p>where:</p>
<figure>
<img src="http://docs.aws.amazon.com/sagemaker/latest/dg/images/FM4.jpg" alt="" /><figcaption>[An image containing the logistic function of the predicted values.]</figcaption>
</figure>
<p>For more information about loss functions for classification, see <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">Loss functions for classification</a>.</p>
</body>
</html>
